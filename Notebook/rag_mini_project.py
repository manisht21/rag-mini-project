# -*- coding: utf-8 -*-
"""rag_mini_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17YRRriXujRLHekDHj9rQ3nIRsZrT-4hD

# Task
Build a Retrieval Augmented Generation (RAG) system using company policy documents (PDF/TXT/Markdown) to answer questions, covering document loading, cleaning, chunking, embedding, vector database storage, semantic retrieval, prompt engineering, and evaluation.

**Mount Google Drive**
"""

from google.colab import drive
drive.mount('/content/drive')

"""**Create Project Folder Structure**"""

import os

base_path = "/content/drive/MyDrive/rag_project"
data_path = os.path.join(base_path, "data")
notebook_path = os.path.join(base_path, "notebook")

os.makedirs(data_path, exist_ok=True)
os.makedirs(notebook_path, exist_ok=True)

print("Created folders:")
print(data_path)
print(notebook_path)

"""**Verify Uploaded Policy Documents**"""

import os

data_path = "/content/drive/MyDrive/rag_project/data"
print(os.listdir(data_path))

"""**Install DOCX Loader Dependency**"""

pip install docx2txt

!pip install python-docx

from docx import Document
import os

data_folder = "/content/drive/MyDrive/rag_project/data"
documents = []

for file in os.listdir(data_folder):
    if file.endswith(".docx"):
        doc = Document(os.path.join(data_folder, file))
        documents.append("\n".join(p.text for p in doc.paragraphs))

print(f"Loaded {len(documents)} documents automatically.")

"""**Install LangChain and Community Packages**"""

pip install -U langchain langchain-community docx2txt

"""**Import Document Loader**"""

from langchain_community.document_loaders import Docx2txtLoader

"""**Load Policy Documents**"""

from langchain_core.documents import Document as LCDocument

docs = [LCDocument(page_content=text) for text in documents]

print(f"Converted {len(docs)} documents into LangChain format.")

"""**Import Text Splitter**"""

from langchain_text_splitters import RecursiveCharacterTextSplitter

"""**Chunk Documents into Smaller Pieces**"""

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)

chunks = splitter.split_documents(docs)
print("Chunks created:", len(chunks))

"""## Design Trade-offs

### Vector Database Choice
ChromaDB was selected for simplicity and fast prototyping. FAISS could offer better performance for very large datasets, but ChromaDB provides easier integration and persistence.

### Chunk Size Choice
A chunk size of 500 characters balances semantic completeness and retrieval precision. Smaller chunks improve recall but increase embedding count, while larger chunks may mix multiple topics.

### Embedding Model
Lightweight sentence-transformer embeddings were used to reduce memory usage and inference cost.

### LLM Choice
flan-t5-base was selected as an open-source model to avoid API costs. Larger models may improve answer quality but require higher compute.

### Notebook Design
A single notebook simplifies demonstration, while modular Python files would improve maintainability for production systems.

**Initialize Embedding Model**
"""

from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

"""**Install Chroma Vector Database**"""

pip install chromadb

"""**Create Vector Store and Retriever**"""

def build_vector_store(chunks, embeddings):
    from langchain_community.vectorstores import Chroma
    db = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory="./chroma_db"
    )
    return db.as_retriever(search_kwargs={"k":4})

"""**File Verification**"""

def verify_files():
    import os
    files = os.listdir("/content/drive/MyDrive/rag_project/data")
    print("Found files:", files)

verify_files()

"""**Logging**"""

import datetime

def log_query(question):
    t = datetime.datetime.now().strftime("%H:%M:%S")
    print(f"[{t}] Question:", question)

def retrieve_context(question):
    docs = retriever.invoke(question)
    return "\n".join([d.page_content for d in docs])

def ask(question):
    log_query(question)
    context = retrieve_context(question)

    if len(context.strip()) == 0:
        return "Not found in the provided documents."

    return generate_answer(context, question)

"""**Define Final Prompt Template**"""

PROMPT = """
You are an AI assistant that answers strictly from company policy documents.

Rules:
1. Use ONLY the information in <context>.
2. If answer not found, say:
   Not found in the provided documents.
3. Do NOT guess or use outside knowledge.
4. Cite evidence.

<context>
{context}
</context>

Question:
{question}

Answer Format:
- Answer:
- Evidence:
"""

"""**Install Transformers for Free LLM**"""

pip install transformers accelerate

"""**Load Open-Source Language Model (FLAN-T5)**"""

from transformers import pipeline

llm = pipeline(
    "text2text-generation",
    model="google/flan-t5-base",
    max_new_tokens=256
)

"""**Define Answer Generation Function**"""

def generate_answer(context, question):
    prompt = PROMPT.format(context=context, question=question)
    output = llm(prompt)[0]["generated_text"]
    return output

def run_evaluation(questions):
    results = []
    for q in questions:
        ans = ask(q)
        results.append({"question": q, "answer": ans})
    return results

"""**RAG CREATE CHECK CODE**"""

retriever = build_vector_store(chunks, embeddings)
print("Retriever created")

"""**Test RAG System with Sample Questions**"""

print(ask("What is the refund period?"))
print(ask("How long does international shipping take?"))
print(ask("Who is the CEO?"))

"""**Run Evaluation on Multiple Questions**"""

evaluation_questions = [
    "What is the refund period?",
    "How long does international shipping take?",
    "Is cancellation allowed after shipping?",
    "Are digital products refundable?",
    "Do you ship to Germany?",
    "What payment gateway is used?"
]

evaluation_results = run_evaluation(evaluation_questions)


for r in evaluation_results:
    print("\nQ:", r["question"])
    print("A:", r["answer"])